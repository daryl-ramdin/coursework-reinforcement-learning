{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3kAR7ClqV6CWbQr69oOfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daryl-ramdin/inm707-daryl-ramdin/blob/main/mountaincar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Packages"
      ],
      "metadata": {
        "id": "4Z2w7iCmxr_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gymnasium"
      ],
      "metadata": {
        "id": "lDSw79mrxklK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "xls-dkAsxWk0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SgWC2ZYFxKKq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #INM707 Lab 8\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU stuff"
      ],
      "metadata": {
        "id": "NrjCkFy4xcpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\",device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VGZsdgcxe6e",
        "outputId": "2d511ced-9d9f-45ba-efd6-9037672536e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameter Settings"
      ],
      "metadata": {
        "id": "XkXgIevPyEGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPSILON = 0.2\n",
        "BUFFER_SIZE = 10\n",
        "BATCH_SIZE = 5\n"
      ],
      "metadata": {
        "id": "fRpLdGJ1yDOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class and Function Declarations"
      ],
      "metadata": {
        "id": "XMlUGD2gyKAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "Transition = namedtuple(\"Transition\",(\"state\", \"action\", \"next_state\", \"reward\"))\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    #This is the DQN class\n",
        "    def __init__(self, configuration):\n",
        "        #ref: INM 707 Lab 8 Feedback\n",
        "        super().__init__()\n",
        "        '''\n",
        "        :param configuration: [{in:int, out:int}]\n",
        "        '''\n",
        "        self.layers = []\n",
        "        for layer in configuration:\n",
        "            self.layers.append(nn.Linear(in_features=layer[\"in\"],out_features=layer[\"out\"]))\n",
        "\n",
        "    def forward(self,input):\n",
        "        #Run the forward pass. ref: INM707 Lab 8\n",
        "        i = 0\n",
        "        for i in range(len(self.layers)-1):\n",
        "            input = F.relu(self.layers[i](input))\n",
        "        #return the output\n",
        "        output = self.layers[i+1](input)\n",
        "        return output\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self,buffer_size,batch_size):\n",
        "        #ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "        self.buffer = deque([],maxlen=buffer_size)\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    def push(self,state,action,next_state,reward):\n",
        "        #ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "        self.buffer.append(Transition(state,action,next_state,reward))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer_size)\n",
        "\n",
        "    def __getitem__(self, item=None):\n",
        "        return random.sample(self.buffer,self.batch_size)\n",
        "\n",
        "\n",
        "\n",
        "def get_next_action(state):\n",
        "    #We operate in epsilon greedy\n",
        "    action = None\n",
        "\n",
        "    if np.random.uniform() > EPSILON:\n",
        "        #We use the best action\n",
        "        action = q_net(torch.tensor(state,device=device,dtype=torch.float))\n",
        "    else:\n",
        "        #We explore\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "    return action\n",
        "    \n",
        "def train():\n",
        "    return\n",
        "\n",
        "def update_target():\n",
        "    return"
      ],
      "metadata": {
        "id": "D7MdRhXDyPTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "gV5ozeBlywbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create our environment\n",
        "env = gym.make('MountainCar-v0')\n",
        "\n",
        "#Create our replay buffer\n",
        "#Let's get the observation. \n",
        "#ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "replay_buffer = ReplayBuffer(buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE)\n",
        "\n",
        "#Let's create the configuration for our network\n",
        "#ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "sizeof_obs = len(env.observation_space)\n",
        "sizeof_actn = len(env.action_space)\n",
        "\n",
        "network_config = [{\"in\":sizeof_obs,\"out\":256},\n",
        "          {\"in\":256,\"out\":256},\n",
        "          {\"in\":256,\"out\":256},\n",
        "          {\"in\":256,\"out\":sizeof_actn}]\n",
        "\n",
        "q_net = DQN(network_config).to(device)\n",
        "tgt_net = DQN(network_config).to(device)\n",
        "tgt_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "\n",
        "episode_count = 1\n",
        "\n",
        "#Training loop\n",
        "for i in range(episode_count):\n",
        "    #ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "    # reset the environment\n",
        "    state, info = env.reset()\n",
        "\n",
        "    while 1:\n",
        "\n",
        "        #Get the action\n",
        "        action = get_next_action(state)\n",
        "\n",
        "        #Get the transition for the action, ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "        observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        next_state = observation\n",
        "\n",
        "        #Store the transition\n",
        "        replay_buffer.push(state, action, next_state, truncated)\n",
        "\n",
        "        #Let our model train on a batch of transitions\n",
        "        train()\n",
        "\n",
        "        state = next_state\n",
        "        break\n",
        "\n",
        "    #Update the weights for the target network\n",
        "    update_target()\n"
      ],
      "metadata": {
        "id": "uJ8QYHzgy1mE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}